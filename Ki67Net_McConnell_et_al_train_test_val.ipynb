{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Ki67Net_McConnell-et-al_train-test-val.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.4"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/richardgault/Automated-Ki-67-proliferation-scoring/blob/main/Ki67Net_McConnell_et_al_train_test_val.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Vg1EEvokWZzz"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rbMqB4cxeQ4b"
      },
      "source": [
        "Note: It is recommended that the path to your data directory contains no spaces"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oLNB7MfqfEIP"
      },
      "source": [
        "data_directory = input('Specify the directory where your data is stored (ending with / e.g drive/MyDrive/myfiles/are/here/): ')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "97NLQHgIoEGJ"
      },
      "source": [
        "import os, cv2, io\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import random, tqdm\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "# Use !pip when running notebook on Google Colab\n",
        "!pip install segmentation-models-pytorch==0.1.3\n",
        "!pip install -U segmentation-models-pytorch\n",
        "!pip install albumentations==0.5.2\n",
        "#!pip install pytorch_toolbelt\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import DataLoader\n",
        "import albumentations as album\n",
        "import segmentation_models_pytorch as smp\n",
        "from PIL import Image, ImageDraw, ImageFilter\n",
        "import gc\n",
        "from torchsummary import summary\n",
        "import csv\n",
        "\n",
        "from sklearn.metrics import mean_squared_error, confusion_matrix, classification_report\n",
        "from math import sqrt\n",
        "import base64\n",
        "import imutils\n",
        "import cv2\n",
        "import torchvision.transforms as transforms"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6UEB_H_rhNV9"
      },
      "source": [
        "# Dataset class\n",
        "class CellDataset(torch.utils.data.Dataset):\n",
        "    \"\"\"Read images from directory, apply augmentation and preprocessing.\n",
        "    \n",
        "    Arguments\n",
        "        images_dir (str): path to images folder\n",
        "        masks_dir (str): path to segmentation masks folder\n",
        "        class_rgb_values (list): RGB values of select classes to extract from segmentation mask\n",
        "        augmentation (albumentations.Compose): data transformation pipeline \n",
        "        preprocessing (albumentations.Compose): data preprocessing \n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(\n",
        "        self, \n",
        "        images_dir, \n",
        "        masks_dir, \n",
        "        class_rgb_values=None, \n",
        "        augmentation=None, \n",
        "        preprocessing=None,\n",
        "    ):\n",
        "        \n",
        "        self.image_paths = [os.path.join(images_dir, image_id) for image_id in sorted(os.listdir(images_dir))]\n",
        "        self.mask_paths = [os.path.join(masks_dir, image_id) for image_id in sorted(os.listdir(masks_dir))]\n",
        "        self.class_rgb_values = class_rgb_values\n",
        "        self.augmentation = augmentation\n",
        "        self.preprocessing = preprocessing\n",
        "    \n",
        "    def __getitem__(self, i):\n",
        "        \n",
        "        # read images and masks\n",
        "        image = cv2.cvtColor(cv2.imread(self.image_paths[i]), cv2.COLOR_BGR2RGB)\n",
        "        mask = cv2.cvtColor(cv2.imread(self.mask_paths[i]), cv2.COLOR_BGR2RGB)\n",
        "        \n",
        "        # encode the mask\n",
        "        mask = encode(mask, self.class_rgb_values).astype('float')\n",
        "        \n",
        "        # apply augmentations\n",
        "        if self.augmentation:\n",
        "            sample = self.augmentation(image=image, mask=mask)\n",
        "            image, mask = sample['image'], sample['mask']\n",
        "        \n",
        "        # apply preprocessing\n",
        "        if self.preprocessing:\n",
        "            sample = self.preprocessing(image=image, mask=mask)\n",
        "            image, mask = sample['image'], sample['mask']\n",
        "            \n",
        "        return image, mask\n",
        "        \n",
        "    def __len__(self):\n",
        "        return len(self.image_paths)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D4ETXZ79hHu2"
      },
      "source": [
        "# Perform colour coding on the reverse-encoded outputs\n",
        "def segmentation(image, label_values):\n",
        "    \"\"\"\n",
        "    Given a 1-channel array of class keys, colour code the segmentation results.\n",
        "    # Arguments\n",
        "        image: single channel array where each value represents the class key.\n",
        "        label_values\n",
        "\n",
        "    # Returns\n",
        "        Colour coded image for segmentation visualization\n",
        "    \"\"\"\n",
        "    colour_codes = np.array(label_values)\n",
        "    x = colour_codes[image.astype(int)]\n",
        "    return x"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SuMw7YZxXI5i"
      },
      "source": [
        "def count_cells(mask):\n",
        "  #mask = np.transpose(mask,(1,2,0))\n",
        "  #select_class_rgb_values = [[0, 0, 0],[128, 128, 128],[255, 255, 255]]\n",
        "  #mask = segmentation(reverse_encode(mask), select_class_rgb_values)\n",
        "\n",
        "  # Convert image to base64\n",
        "  output_img = Image.fromarray(mask.astype(\"uint8\"))\n",
        "  rawBytes = io.BytesIO()\n",
        "  output_img.save(rawBytes, \"PNG\")\n",
        "  rawBytes.seek(0)\n",
        "  img_base64 = base64.b64encode(rawBytes.read()).decode('utf-8')\n",
        "\n",
        "  tensor_to_img = transforms.ToPILImage()(mask.astype(\"uint8\")).convert(\"RGB\")\n",
        "  gray = cv2.cvtColor(np.array(tensor_to_img),cv2.COLOR_RGB2GRAY)\n",
        "  thresh_gaussian = cv2.adaptiveThreshold(gray, 255, cv2.ADAPTIVE_THRESH_GAUSSIAN_C,cv2.THRESH_BINARY, 199, 5)                                      \n",
        "\n",
        "  # Find the contours\n",
        "  contours = cv2.findContours(thresh_gaussian.copy(), cv2.RETR_TREE, cv2.CHAIN_APPROX_SIMPLE)\n",
        "  contours = imutils.grab_contours(contours)\n",
        "\n",
        "  # Count number of healthy and Ki67 cells\n",
        "  ki67_counter = 0\n",
        "  normal_cell_counter = 0\n",
        "  for c in contours:\n",
        "      M = cv2.moments(c)\n",
        "      if(M[\"m00\"]!=0):\n",
        "          cX = int(M[\"m10\"] / M[\"m00\"])\n",
        "          cY = int(M[\"m01\"] / M[\"m00\"])\n",
        "          if tensor_to_img.getpixel((cX,cY)) == (255,255,255):\n",
        "              ki67_counter = ki67_counter + 1\n",
        "          elif tensor_to_img.getpixel((cX,cY)) == (128,128,128):\n",
        "              normal_cell_counter = normal_cell_counter + 1\n",
        "          # cX = int(M[\"m10\"] / M[\"m00\"])\n",
        "          # cY = int(M[\"m01\"] / M[\"m00\"])\n",
        "          # if gray[cX,cY] == 255:\n",
        "          #     ki67_counter = ki67_counter + 1\n",
        "          # elif gray[cX,cY] == 128:\n",
        "          #     normal_cell_counter = normal_cell_counter + 1\n",
        "      else:\n",
        "          cX,cY = 0,0\n",
        "  return normal_cell_counter,ki67_counter"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e-lQUN-LhCOi"
      },
      "source": [
        "def write_csv(output_name,data_array):\n",
        "  with open(output_name, 'w', newline='') as file:\n",
        "    writer = csv.writer(file)\n",
        "    writer.writerow([\"GT Normal cells\", \"GT Ki67 cells\",\"Pred Normal cells\", \"Pred Ki67 cells\"])\n",
        "    for x in data_array:      \n",
        "      writer.writerow(x)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A89dlrvNky8a"
      },
      "source": [
        "def boundary_confusion_matrix(all_counts):\n",
        "  gt_grouping = []\n",
        "  pred_groupings = []\n",
        "  for c in all_counts:\n",
        "    if (c[1]/(c[0]+c[1]))<=0.1: \n",
        "      gt_grouping.append(1);\n",
        "    elif (c[1]/(c[0]+c[1]))<=0.2:\n",
        "      gt_grouping.append(2);\n",
        "    else:\n",
        "      gt_grouping.append(3);\n",
        "\n",
        "    if (c[3]/(c[2]+c[3]))<=0.1:\n",
        "      pred_groupings.append(1);\n",
        "    elif (c[3]/(c[2]+c[3]))<=0.2:\n",
        "      pred_groupings.append(2);\n",
        "    else:\n",
        "      pred_groupings.append(3);\n",
        " \n",
        "  #print(gt_grouping)\n",
        "  #print(pred_groupings)\n",
        "  print(\"--------------------\")\n",
        "  print(\"Boundary Matrix\")\n",
        "  print(confusion_matrix(gt_grouping, pred_groupings))\n",
        "  print(classification_report(gt_grouping, pred_groupings, digits=3))\n",
        "  print(\"--------------------\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b0buYpYIg66w"
      },
      "source": [
        "# Helper function for data visualisation\n",
        "def visualize(**images):\n",
        "    \"\"\"\n",
        "    Plot images horizontally\n",
        "    \"\"\"\n",
        "    n_images = len(images)\n",
        "    plt.figure(figsize=(20,8))\n",
        "    for idx, (name, image) in enumerate(images.items()):\n",
        "        plt.subplot(1, n_images, idx + 1)\n",
        "        plt.xticks([]); \n",
        "        plt.yticks([])\n",
        "        # get title from the parameter names\n",
        "        plt.title(name.replace('_',' ').title(), fontsize=20)\n",
        "        plt.imshow(image)\n",
        "    plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eco3Net_g9wG"
      },
      "source": [
        "# Perform encoding on label (mask to one-hot)\n",
        "def encode(label, label_values):\n",
        "    \"\"\"\n",
        "    Convert a segmentation mask (H, W, C) to (H, W, K) where the last dimension is a one\n",
        "    hot encoding vector, where C = number of channels, and K = number of classes.\n",
        "    Replaces each pixel value with a vector of length num_classes\n",
        "\n",
        "    # Arguments\n",
        "        label: The 2D array segmentation image label\n",
        "        label_values\n",
        "        \n",
        "    # Returns\n",
        "        A 2D array with the same width and hieght as the input, but with a depth size of num_classes\n",
        "    \"\"\"\n",
        "    semantic_map = []\n",
        "    for colour in label_values:\n",
        "        equality = np.equal(label, colour)\n",
        "        class_map = np.all(equality, axis = -1)\n",
        "        semantic_map.append(class_map)\n",
        "    semantic_map = np.stack(semantic_map, axis=-1)\n",
        "    return semantic_map"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rbJwVBLXg__2"
      },
      "source": [
        "# Perform reverse encoding on labels or predictions (one-hot to mask)\n",
        "def reverse_encode(image):\n",
        "    \"\"\"\n",
        "    Transform a 2D array in encoded format (depth is num_classes),\n",
        "    to a 2D array with only 1 channel, where each pixel value is\n",
        "    the classified class key. \n",
        "    Convert a mask (H, W, K) to (H, W, C), where K = number of classes and C = number of channels\n",
        "    # Arguments\n",
        "        image: The encoded image \n",
        "        \n",
        "    # Returns\n",
        "        A 2D array with the same width and height as the input, but\n",
        "        with a depth size of 1, where each pixel value is the classified \n",
        "        class key.\n",
        "    \"\"\"\n",
        "    x = np.argmax(image, axis = -1)\n",
        "    return x\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HMkHEu1ci4--"
      },
      "source": [
        "# Data Pre-processing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ayqD2-Dqi2lx"
      },
      "source": [
        "# Apply preprocessing\n",
        "def get_preprocessing(preprocessing_fn=None):\n",
        "    \"\"\"Construct preprocessing transform    \n",
        "    Arguments\n",
        "        preprocessing_fn : data normalization function \n",
        "    Returns\n",
        "        transform: albumentations.Compose\n",
        "    \"\"\"   \n",
        "    if preprocessing_fn:\n",
        "      _transform = [\n",
        "        album.Lambda(image=preprocessing_fn),\n",
        "        album.Lambda(image=to_tensor, mask=to_tensor),\n",
        "    ]\n",
        "    else:\n",
        "      _transform = [\n",
        "        album.Lambda(image=to_tensor, mask=to_tensor),\n",
        "    ]\n",
        "        \n",
        "    return album.Compose(_transform)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hgpEgM6ejA2V"
      },
      "source": [
        "# Convert to Tensor\n",
        "def to_tensor(x, **kwargs):\n",
        "    return x.transpose(2, 0, 1).astype('float32')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PVfn1NFqjGFt"
      },
      "source": [
        "# Center crop padded image to original dimensions\n",
        "def crop_image(image, target_image_dims=[256,256,3]):\n",
        "    target_size = target_image_dims[0]\n",
        "    image_size = len(image)\n",
        "    padding = (image_size - target_size) // 2\n",
        "    return image[\n",
        "        padding:image_size - padding,\n",
        "        padding:image_size - padding,\n",
        "        :,\n",
        "    ]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uqqcKGtMiz4V"
      },
      "source": [
        "def get_validation_augmentation():   \n",
        "    # Add padding to ensure image is divisible by 32\n",
        "    test_transform = [\n",
        "        album.PadIfNeeded(min_height=256, min_width=256, always_apply=True, border_mode=0),\n",
        "    ]\n",
        "    return album.Compose(test_transform)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BynmO-xiixTl"
      },
      "source": [
        "# Augmentation methods for dataset\n",
        "\n",
        "def get_training_augmentation():\n",
        "    train_transform = [    \n",
        "        album.OneOf(\n",
        "            [\n",
        "                album.HorizontalFlip(p=1),\n",
        "                album.VerticalFlip(p=1),\n",
        "                album.RandomRotate90(p=1),\n",
        "            ],\n",
        "            p=0.75,\n",
        "        ),\n",
        "    ]\n",
        "    return album.Compose(train_transform)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bNLHiir6jHlU"
      },
      "source": [
        "# Main body of code\n",
        "\n",
        "## Data format\n",
        "\n",
        "It is assumed that within the main data directory (entered previously) that the datasets are in the following directory structure\n",
        "\n",
        "*Dataset directory*\n",
        "\n",
        "*   label_class_dict.csv\n",
        "*   train\n",
        "\n",
        "    ->image1.png\n",
        "\n",
        "    ->image2.png\n",
        "\n",
        "    ->...\n",
        "    \n",
        "*   train_masks\n",
        "\n",
        "    ->image1_mask.png\n",
        "\n",
        "    ->image2_mask.png\n",
        "\n",
        "    ->...\n",
        "    \n",
        "*   test\n",
        "\n",
        "    ->imageT1.png\n",
        "\n",
        "    ->imageT2.png\n",
        "\n",
        "    ->...\n",
        "    \n",
        "*   test_masks\n",
        "\n",
        "    ->imageT1_mask.png\n",
        "\n",
        "    ->imageT2_mask.png\n",
        "\n",
        "    ->...\n",
        "    \n",
        "*   validation\n",
        "\n",
        "    ->imageV1.png\n",
        "\n",
        "    ->imageV2.png\n",
        "\n",
        "    ->...\n",
        "    \n",
        "*   validation_masks\n",
        "\n",
        "    ->imageV1_mask.png\n",
        "\n",
        "    ->imageV2_mask.png\n",
        "    \n",
        "    ->..."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yS-CGl8ubSo2"
      },
      "source": [
        "#Note\n",
        "\n",
        "label_class_dict.csv must be saved in the same directory as the data."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cDaklLdmrbrr"
      },
      "source": [
        "results_directory = data_directory+'results/'\n",
        "if not os.path.isdir(results_directory):  \n",
        "  os.mkdir(results_directory)\n",
        "  print('Created a new Results folder')\n",
        "training_images = os.path.join(data_directory, 'train_png')\n",
        "training_labels = os.path.join(data_directory, 'train_masks')\n",
        "validation_images = os.path.join(data_directory, 'validation_png')\n",
        "validation_labels = os.path.join(data_directory, 'validation_masks')\n",
        "test_images = os.path.join(data_directory, 'test_png')\n",
        "test_labels = os.path.join(data_directory, 'test_masks')\n",
        "\n",
        "class_dict = pd.read_csv(data_directory+\"label_class_dict.csv\")\n",
        "class_names = class_dict['name'].tolist()\n",
        "class_rgb_values = class_dict[['r','g','b']].values.tolist()\n",
        "\n",
        "select_classes = ['background', 'cell_healthy', 'cell_ki67']\n",
        "select_class_rgb_values =   [[  0,   0,   0], [128, 128, 128], [255, 255, 255]]\n",
        "print('Dataset classes: ', select_classes)\n",
        "print('RGB values: ', select_class_rgb_values)\n",
        "    \n",
        "# Create segmentation model with pretrained ResNet50 encoder as backbone\n",
        "\n",
        "encoder = 'resnet50'\n",
        "encoder_weights = 'imagenet'\n",
        "classes = class_names\n",
        "activation = 'softmax' \n",
        "\n",
        "model = smp.Unet(\n",
        "    encoder_name=encoder, \n",
        "    encoder_weights=encoder_weights, \n",
        "    classes=len(classes), \n",
        "    activation=activation,\n",
        ")\n",
        "\n",
        "preprocessing_fn = smp.encoders.get_preprocessing_fn(encoder, encoder_weights)\n",
        "\n",
        "# Initialising training and validation data loaders\n",
        "train_dataset = CellDataset(\n",
        "    training_images, training_labels, \n",
        "    augmentation=get_training_augmentation(),\n",
        "    preprocessing=get_preprocessing(preprocessing_fn),\n",
        "    class_rgb_values=select_class_rgb_values,\n",
        ")\n",
        "\n",
        "valid_dataset = CellDataset(\n",
        "    validation_images, validation_labels, \n",
        "    augmentation=get_validation_augmentation(), \n",
        "    preprocessing=get_preprocessing(preprocessing_fn),\n",
        "    class_rgb_values=select_class_rgb_values,\n",
        ")\n",
        "\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=8, shuffle=True, num_workers=0)\n",
        "valid_loader = DataLoader(valid_dataset, batch_size=1, shuffle=False, num_workers=0)\n",
        "\n",
        "dataset = CellDataset(training_images, training_labels, class_rgb_values=select_class_rgb_values)\n",
        "\n",
        "print(\"\\nNumber of training images:\",len(os.listdir(training_images)))\n",
        "print(\"\\nNumber of training masks:\",len(os.listdir(training_labels)))\n",
        "print(\"\\nNumber of training validation images:\",len(os.listdir(validation_images)))\n",
        "print(\"\\nNumber of training validation masks:\",len(os.listdir(validation_labels)))\n",
        "# Set variables for model\n",
        "\n",
        "trainingMode = True   # Flag for running model training, 'False' = only prediction is performed\n",
        "epochs = 10\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(\"\\nDevice being used:\\n\", device)\n",
        "model = model.to(device)\n",
        "loss = smp.utils.losses.DiceLoss()\n",
        "\n",
        "print(\"\\nMODEL INFO:\\n\", model)\n",
        "print(\"\\nMODEL SUMMARY:\\n\")\n",
        "summary(model, input_size=(3, 256, 256))\n",
        "\n",
        "metrics = [\n",
        "    smp.utils.metrics.Accuracy(),\n",
        "    smp.utils.metrics.Precision(),\n",
        "    smp.utils.metrics.Recall(),\n",
        "    smp.utils.metrics.IoU(threshold=0.5),\n",
        "    smp.utils.metrics.Fscore(),\n",
        "]\n",
        "\n",
        "optimizer = torch.optim.Adam([dict(params=model.parameters(), lr=0.0001),])\n",
        "    \n",
        "train_epoch = smp.utils.train.TrainEpoch(\n",
        "    model, \n",
        "    loss=loss, \n",
        "    metrics=metrics, \n",
        "    optimizer=optimizer,\n",
        "    device=device,\n",
        "    verbose=True,\n",
        ")\n",
        "\n",
        "valid_epoch = smp.utils.train.ValidEpoch(\n",
        "    model, \n",
        "    loss=loss, \n",
        "    metrics=metrics, \n",
        "    device=device,\n",
        "    verbose=True,\n",
        ")\n",
        "\n",
        "model_path = results_directory+'best_model.pth'\n",
        "model_dictionary_path = results_directory+'model_dictionary.pth'\n",
        "\n",
        "best_iou_score = 0.0\n",
        "      \n",
        "# Load best saved model checkpoint from the current run\n",
        "if os.path.exists(model_path):\n",
        "  best_model = torch.load(model_path, map_location=device)\n",
        "  print('Loaded model from current run')\n",
        "\n",
        "\n",
        "train_logs_list, valid_logs_list = [], []\n",
        "\n",
        "if trainingMode:\n",
        "  for i in range(epochs):\n",
        "    # Perform training & validation\n",
        "    print('\\nEpoch (real-life dataset): {}'.format(i))\n",
        "    train_logs = train_epoch.run(train_loader)\n",
        "    validation_logs = valid_epoch.run(valid_loader)\n",
        "    train_logs_list.append(train_logs)\n",
        "    valid_logs_list.append(validation_logs)\n",
        "\n",
        "    # Save model and dictionary if a better IoU score is obtained\n",
        "    if best_iou_score < validation_logs['iou_score']:\n",
        "      best_iou_score = validation_logs['iou_score']\n",
        "      torch.save(model, model_path)\n",
        "      torch.save(model.state_dict(), model_dictionary_path)\n",
        "      torch.save(model.state_dict(), results_directory+\"model_dictionary-new.pth\")\n",
        "      print('Model and dictionary saved')\n",
        "\n",
        "      # Optimising memory usage for GPU    \n",
        "      gc.collect()\n",
        "      torch.cuda.empty_cache()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z3F8iDcfd4av"
      },
      "source": [
        "#Testing Section"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fr9qZEZmf0FH"
      },
      "source": [
        "print(\"\\nNumber of test images:\",len(os.listdir(test_images)))\n",
        "print(\"\\nNumber of test masks:\",len(os.listdir(test_labels)))\n",
        "\n",
        "# Load best saved model checkpoint from the current run\n",
        "if os.path.exists(model_path):\n",
        "    best_model = torch.load(model_path, map_location=device)\n",
        "    print('Loaded model from current run')\n",
        "\n",
        "# Initialising test dataloader \n",
        "test_dataset = CellDataset(\n",
        "    test_images, \n",
        "    test_labels, \n",
        "    augmentation=get_validation_augmentation(),\n",
        "    preprocessing=get_preprocessing(preprocessing_fn),\n",
        "    class_rgb_values=select_class_rgb_values,\n",
        ")\n",
        "\n",
        "test_dataloader = DataLoader(test_dataset)\n",
        "\n",
        "# Test dataset for visualization (without preprocessing transformations)\n",
        "test_dataset_vis = CellDataset(\n",
        "    test_images, test_labels, \n",
        "    augmentation=get_validation_augmentation(), \n",
        "    class_rgb_values=select_class_rgb_values,\n",
        ")\n",
        "\n",
        "sample_predictions_directory = results_directory+'sample_predictions/'\n",
        "if not os.path.exists(sample_predictions_directory):\n",
        "    os.makedirs(sample_predictions_directory)\n",
        "    print(['Made directory for predictions at: ',sample_predictions_directory])\n",
        "    \n",
        "# Display predicted masks for 10 of the test images from the real-life dataset\n",
        "all_counts = [];\n",
        "for idx in np.arange(min(len(test_dataset),10)):#...or all data with len(test_dataset_real)):\n",
        "    image, groundTruth_mask = test_dataset[idx]\n",
        "    image_visual = crop_image(test_dataset_vis[idx][0].astype('uint8'))\n",
        "    image_tensor = torch.from_numpy(image).to(device).unsqueeze(0)\n",
        "    # Predict test image\n",
        "    pred_mask = best_model(image_tensor)\n",
        "    # output = pred_mask.squeeze(0)\n",
        "    # output_predictions = output.argmax(0)\n",
        "    # print(\"output predictions\",output_predictions)\n",
        "    pred_mask = pred_mask.detach().squeeze().cpu().numpy()\n",
        "    # Convert prediction mask from CHW format to HWC format\n",
        "    pred_mask = np.transpose(pred_mask,(1,2,0))\n",
        "    # Get prediction channel corresponding to Ki67\n",
        "    pred_Ki67_heatmap = pred_mask[:,:,select_classes.index('cell_ki67')]\n",
        "    pred_mask = crop_image(segmentation(reverse_encode(pred_mask), select_class_rgb_values))\n",
        "\n",
        "    # Convert ground truth mask from CHW format to HWC format\n",
        "    groundTruth_mask = np.transpose(groundTruth_mask,(1,2,0))\n",
        "    groundTruth_mask = crop_image(segmentation(reverse_encode(groundTruth_mask), select_class_rgb_values))\n",
        "    cv2.imwrite(os.path.join(sample_predictions_directory, f\"sample_pred_{idx+1}.png\"), np.hstack([image_visual, groundTruth_mask, pred_mask])[:,:,::-1])\n",
        "    \n",
        "    # Get ground truth counts\n",
        "    GT_normal_cell_counter,GT_ki67_counter = count_cells(groundTruth_mask)\n",
        "    #Get predicted counts\n",
        "    Pred_normal_cell_counter,Pred_ki67_counter = count_cells(pred_mask)\n",
        "    all_counts.append([GT_normal_cell_counter,GT_ki67_counter,Pred_normal_cell_counter,Pred_ki67_counter])\n",
        "\n",
        "    if idx<25:\n",
        "      visualize(\n",
        "        original_image = image_visual,\n",
        "        ground_truth_mask = groundTruth_mask,\n",
        "        predicted_mask = pred_mask,\n",
        "        predicted_Ki67_heatmap = pred_Ki67_heatmap\n",
        "        )"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PwX1EEvpWda3"
      },
      "source": [
        "#print summary of counts\n",
        "normal_GT = [r[0] for r in all_counts]\n",
        "Ki67_GT =[r[1] for r in all_counts]\n",
        "normal_pred = [r[2] for r in all_counts]\n",
        "Ki67_pred = [r[3] for r in all_counts]\n",
        "print([\"RMSE Normal \", sqrt(mean_squared_error(normal_GT, normal_pred))])\n",
        "print([\"RMSE Ki67 \", sqrt(mean_squared_error(Ki67_GT, Ki67_pred))])\n",
        "print([\"RMSE ratio \", sqrt(mean_squared_error(np.divide(Ki67_GT,np.add(normal_GT,Ki67_GT)),np.divide(Ki67_pred,np.add(normal_pred,Ki67_pred))))])\n",
        "boundary_confusion_matrix(all_counts)\n",
        "#save counts to file\n",
        "write_csv(results_directory+\"count_results.csv\",all_counts)\n",
        "# Perform testing\n",
        "\n",
        "test_epoch = smp.utils.train.ValidEpoch(\n",
        "    model,\n",
        "    loss=loss, \n",
        "    metrics=metrics, \n",
        "    device=device,\n",
        "    verbose=True,\n",
        ")\n",
        "\n",
        "\n",
        "\n",
        "testing_logs = test_epoch.run(test_dataloader)\n",
        "test_logs_list = []\n",
        "test_logs_list.append(testing_logs)\n",
        "\n",
        "print(\"\\nEvaluating test data for images: \")\n",
        "print(f\"Mean Dice Loss: {testing_logs['dice_loss']:.4f}\")\n",
        "print(f\"Mean IoU Score: {testing_logs['iou_score']:.4f}\")\n",
        "print(f\"Mean Accuracy: {testing_logs['accuracy']:.4f}\")\n",
        "print(f\"Mean Precision: {testing_logs['precision']:.4f}\")\n",
        "print(f\"Mean Recall: {testing_logs['recall']:.4f}\")\n",
        "\n",
        "# Convert data lists to dataframes, then transpose index and columns\n",
        "train_logs_df = pd.DataFrame(train_logs_list)\n",
        "train_validation_logs_df = pd.DataFrame(valid_logs_list)\n",
        "testing_logs_df = pd.DataFrame(test_logs_list)\n",
        "train_logs_df.T \n",
        "print(\"\\nTraining data summary\\n\",train_logs_df)\n",
        "print(\"\\nValidation during training data summary\\n\",train_validation_logs_df)\n",
        "print(\"\\nTesting data summary\\n\",testing_logs_df)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LzYpJp43f5eg"
      },
      "source": [
        "# Visualisation of model performance comparing different metrics for real-life dataset\n",
        "\n",
        "plt.figure(figsize=(20,8))\n",
        "plt.plot(train_logs_df.index.tolist(), train_logs_df.iou_score.tolist(), lw=3, label = 'Training')\n",
        "plt.plot(train_validation_logs_df.index.tolist(), train_validation_logs_df.iou_score.tolist(), lw=3, label = 'Validation during training')\n",
        "plt.xlabel('Epochs', fontsize=16)\n",
        "plt.ylabel('IoU', fontsize=16)\n",
        "plt.xticks(fontsize=12)\n",
        "plt.yticks(fontsize=12)\n",
        "plt.title('A plot of IoU against Epochs for real-life dataset', fontsize=17)\n",
        "plt.legend(loc='best', fontsize=16)\n",
        "plt.grid()\n",
        "plt.savefig('iou_score_plot.png')\n",
        "plt.show()\n",
        "\n",
        "plt.figure(figsize=(20,8))\n",
        "plt.plot(train_logs_df.index.tolist(), train_logs_df.accuracy.tolist(), lw=3, label = 'Training')\n",
        "plt.plot(train_validation_logs_df.index.tolist(), train_validation_logs_df.accuracy.tolist(), lw=3, label = 'Validation during training')\n",
        "plt.xlabel('Epochs', fontsize=16)\n",
        "plt.ylabel('Accuracy', fontsize=16)\n",
        "plt.xticks(fontsize=12)\n",
        "plt.yticks(fontsize=12)\n",
        "plt.title('A plot of Accuracy against Epochs for real-life dataset', fontsize=17)\n",
        "plt.legend(loc='best', fontsize=16)\n",
        "plt.grid()\n",
        "plt.savefig('accuracy_plot.png')\n",
        "plt.show()\n",
        "\n",
        "plt.figure(figsize=(20,8))\n",
        "plt.plot(train_logs_df.index.tolist(), train_logs_df.precision.tolist(), lw=3, label = 'Training')\n",
        "plt.plot(train_validation_logs_df.index.tolist(), train_validation_logs_df.precision.tolist(), lw=3, label = 'Validation during training')\n",
        "plt.xlabel('Epochs', fontsize=16)\n",
        "plt.ylabel('Precision', fontsize=16)\n",
        "plt.xticks(fontsize=12)\n",
        "plt.yticks(fontsize=12)\n",
        "plt.title('A plot of Precision against Epochs for real-life dataset', fontsize=17)\n",
        "plt.legend(loc='best', fontsize=16)\n",
        "plt.grid()\n",
        "plt.savefig('precision_plot.png')\n",
        "plt.show()\n",
        "\n",
        "plt.figure(figsize=(20,8))\n",
        "plt.plot(train_logs_df.index.tolist(), train_logs_df.recall.tolist(), lw=3, label = 'Training')\n",
        "plt.plot(train_validation_logs_df.index.tolist(), train_validation_logs_df.recall.tolist(), lw=3, label = 'Validation during training')\n",
        "plt.xlabel('Epochs', fontsize=16)\n",
        "plt.ylabel('Recall', fontsize=16)\n",
        "plt.xticks(fontsize=12)\n",
        "plt.yticks(fontsize=12)\n",
        "plt.title('A plot of Recall against Epochs for real-life dataset', fontsize=17)\n",
        "plt.legend(loc='best', fontsize=16)\n",
        "plt.grid()\n",
        "plt.savefig('recall_plot.png')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Rwncrd6OW4W8"
      },
      "source": [
        "boundary_confusion_matrix(all_counts)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}